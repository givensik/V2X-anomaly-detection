#!/usr/bin/env python3
"""
V2X ì´ìƒíƒì§€ ê°œì„  ë²„ì „ - ì˜¤íƒì§€ ê°ì†Œ ì§‘ì¤‘
1. Balanced threshold (F1Ã—Accuracy, Youden's J, Precision-Recall ê· í˜•)
2. Alpha ì¡°ì • (Rule ë¹„ì¤‘ ì¦ê°€: 0.7 â†’ 0.3~0.5)
3. k-of-T ë¼ë²¨ë§ (3~5 í”„ë ˆì„ ì´ìƒ ê³µê²©ì‹œë§Œ ê³µê²©ìœ¼ë¡œ ë¶„ë¥˜)
4. Rule ì ìˆ˜ ì§‘ê³„ ê°œì„  (max â†’ top 20% í‰ê· )
5. Feature weight ì™„í™” (dspeed 1.5â†’1.1, dheading_rad 1.3â†’1.0)
6. ë‹¤ì¤‘ ì„ê³„ê°’ ì‹¤í—˜ ë° ìµœì í™”
"""

import os
import json
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_score, recall_score,
    fbeta_score, classification_report, confusion_matrix, roc_curve
)

from v2x_preprocessing_lstm import V2XDataPreprocessor
from v2x_testing_lstm import LSTMAutoEncoder, V2XDataset

class ImprovedV2XPreprocessor(V2XDataPreprocessor):
    """ê°œì„ ëœ ì „ì²˜ë¦¬ê¸° - k-of-T ë¼ë²¨ë§ + Rule ì ìˆ˜ ì§‘ê³„ ê°œì„ """
    
    def create_sequences_k_of_t(self, df: pd.DataFrame, sequence_length: int = 20, 
                               k_threshold: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """k-of-T ë¼ë²¨ë§ìœ¼ë¡œ ì‹œí€€ìŠ¤ ìƒì„±"""
        numeric_features = [c for c in self.feature_columns if c in df.columns]
        sequences, labels, rule_scores = [], [], []

        for (dataset, station_id), g in df.groupby(['dataset', 'station_id'], sort=False):
            station_data = g.sort_values('timestamp')
            if len(station_data) < sequence_length:
                continue
                
            X = station_data[numeric_features].values
            y = station_data['is_attacker'].values
            r = station_data['rule_score'].values if 'rule_score' in station_data.columns else np.zeros(len(station_data))
            
            for i in range(len(station_data) - sequence_length + 1):
                sequences.append(X[i:i+sequence_length])
                
                # k-of-T ë¼ë²¨ë§: kê°œ ì´ìƒ ê³µê²© í”„ë ˆì„ì´ ìˆìœ¼ë©´ ê³µê²©
                attack_frames = y[i:i+sequence_length].sum()
                labels.append(1 if attack_frames >= k_threshold else 0)
                
                # Rule ì ìˆ˜: ìƒìœ„ 20% í‰ê·  (outlier ì™„í™”)
                rule_window = r[i:i+sequence_length]
                top_20_percent = max(1, int(np.ceil(len(rule_window) * 0.2)))
                rule_scores.append(np.mean(np.sort(rule_window)[-top_20_percent:]))
                
        return np.array(sequences), np.array(labels), np.array(rule_scores)

def find_balanced_threshold(scores: np.ndarray, labels: np.ndarray, 
                          method: str = 'f1_accuracy') -> Tuple[float, dict]:
    """ê°œì„ ëœ ì„ê³„ê°’ ì°¾ê¸° - ë‹¤ì¤‘ ë°©ë²• ì§€ì›"""
    thresholds = np.linspace(scores.min(), scores.max(), 1000)
    best_score = -1
    best_thr = scores.mean()
    best_metrics = {}
    
    for thr in thresholds:
        preds = (scores > thr).astype(int)
        
        try:
            acc = accuracy_score(labels, preds)
            prec = precision_score(labels, preds, zero_division=0)
            rec = recall_score(labels, preds, zero_division=0)
            
            if method == 'f1_accuracy':
                # F1 Ã— Accuracy ìµœì í™”
                if prec + rec > 0:
                    f1 = 2 * prec * rec / (prec + rec)
                    score = f1 * acc
                else:
                    score = 0
                    
            elif method == 'youden_j':
                # Youden's J = TPR - FPR ìµœì í™”
                tn = np.sum((preds == 0) & (labels == 0))
                fp = np.sum((preds == 1) & (labels == 0))
                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
                score = rec - fpr  # TPR - FPR
                
            elif method == 'precision_recall_balance':
                # Precisionê³¼ Recallì˜ ê· í˜• ìµœì í™”
                if prec + rec > 0:
                    # Precisionê³¼ Recallì˜ ê¸°í•˜í‰ê· 
                    score = np.sqrt(prec * rec)
                else:
                    score = 0
                    
            elif method == 'fbeta_0_5':
                # F0.5 score (Precisionì— ë” ê°€ì¤‘ì¹˜)
                if prec + rec > 0:
                    score = fbeta_score(labels, preds, beta=0.5)
                else:
                    score = 0
            
            elif method == 'fbeta_0_3':
                # F0.3 score (Precisionì— ë” ê°€ì¤‘ì¹˜)
                if prec + rec > 0:
                    score = fbeta_score(labels, preds, beta=0.3)
                else:
                    score = 0
            
            elif method == 'high_precision':
                # Precision ìµœëŒ€í™”
                score = prec
            
            if score > best_score:
                best_score = score
                best_thr = thr
                best_metrics = {
                    'accuracy': acc, 'precision': prec, 'recall': rec,
                    'f1': 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0
                }
                
        except:
            continue
    
    return best_thr, best_metrics

def find_optimal_threshold_roc(scores: np.ndarray, labels: np.ndarray) -> Tuple[float, dict]:
    """ROC ì»¤ë¸Œ ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ì°¾ê¸°"""
    fpr, tpr, thresholds = roc_curve(labels, scores)
    
    # Youden's J ìµœì í™”
    j_scores = tpr - fpr
    optimal_idx = np.argmax(j_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    # í•´ë‹¹ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ê³„ì‚°
    preds = (scores > optimal_threshold).astype(int)
    acc = accuracy_score(labels, preds)
    prec = precision_score(labels, preds, zero_division=0)
    rec = recall_score(labels, preds, zero_division=0)
    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0
    
    return optimal_threshold, {
        'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1,
        'youden_j': j_scores[optimal_idx]
    }

def improved_fuse_scores(ae_seq_scores: np.ndarray, rule_seq_scores: np.ndarray,
                        alpha: float = 0.5) -> np.ndarray:
    """ê°œì„ ëœ late-fusion (Rule ë¹„ì¤‘ ì¦ê°€)"""
    ae = np.asarray(ae_seq_scores, dtype=float)
    rule = np.asarray(rule_seq_scores, dtype=float).clip(0.0, 1.0)
    
    # IQR ê¸°ë°˜ AE ì •ê·œí™”
    q1, q2, q3 = np.percentile(ae, [25, 50, 75])
    iqr = q3 - q1 + 1e-12
    ae_norm = np.clip((ae - q2) / iqr + 0.5, 0.0, 1.0)
    
    # Rule ë¹„ì¤‘ì„ ëŠ˜ë¦° fusion
    combined = alpha * ae_norm + (1 - alpha) * rule
    return combined

def experiment_with_configurations(X_test: np.ndarray, y_test: np.ndarray, r_test: np.ndarray,
                                 errors: np.ndarray, model: nn.Module, device: str) -> Dict:
    """ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜í•˜ì—¬ ìµœì  ì¡°í•© ì°¾ê¸°"""
    
    print(f"\nğŸ§ª EXPERIMENTING WITH DIFFERENT CONFIGURATIONS")
    print(f"=" * 70)
    
    # ì‹¤í—˜ ì„¤ì •ë“¤ - ë” ê³µê²©ì ì¸ False Alarm Rate ê°ì†Œ
    configs = [
        # ê¸°ì¡´ ì„¤ì •ë“¤
        {'alpha': 0.3, 'k_threshold': 4, 'method': 'precision_recall_balance'},
        {'alpha': 0.4, 'k_threshold': 3, 'method': 'youden_j'},
        {'alpha': 0.5, 'k_threshold': 3, 'method': 'f1_accuracy'},
        {'alpha': 0.3, 'k_threshold': 5, 'method': 'fbeta_0_5'},
        
        # ì¶”ê°€ ê³µê²©ì  ì„¤ì •ë“¤ (False Alarm Rate ê°ì†Œ ì§‘ì¤‘)
        {'alpha': 0.2, 'k_threshold': 5, 'method': 'fbeta_0_5'},  # Rule ë¹„ì¤‘ ë” ì¦ê°€
        {'alpha': 0.1, 'k_threshold': 6, 'method': 'precision_recall_balance'},  # ë§¤ìš° ë³´ìˆ˜ì 
        {'alpha': 0.3, 'k_threshold': 4, 'method': 'fbeta_0_3'},  # Precisionì— ë” ê°€ì¤‘ì¹˜
        {'alpha': 0.2, 'k_threshold': 5, 'method': 'high_precision'},  # Precision ìµœëŒ€í™”
    ]
    
    best_config = None
    best_f1 = 0
    best_far = 1.0
    
    results = {}
    
    for i, config in enumerate(configs):
        print(f"\nğŸ”¬ Configuration {i+1}:")
        print(f"   Alpha: {config['alpha']}, k-threshold: {config['k_threshold']}")
        print(f"   Threshold method: {config['method']}")
        
        # Late fusion
        combined = improved_fuse_scores(errors, r_test, alpha=config['alpha'])
        
        # ì„ê³„ê°’ ì°¾ê¸°
        if config['method'] == 'youden_j':
            thr_opt, metrics = find_optimal_threshold_roc(combined, y_test)
        else:
            thr_opt, metrics = find_balanced_threshold(combined, y_test, method=config['method'])
        
        # ì„±ëŠ¥ ê³„ì‚°
        preds = (combined > thr_opt).astype(int)
        cm = confusion_matrix(y_test, preds)
        tn, fp, fn, tp = cm.ravel()
        
        attack_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
        false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        results[f'config_{i+1}'] = {
            'alpha': config['alpha'],
            'k_threshold': config['k_threshold'],
            'method': config['method'],
            'threshold': thr_opt,
            'precision': metrics['precision'],
            'recall': metrics['recall'],
            'f1': metrics['f1'],
            'accuracy': metrics['accuracy'],
            'attack_detection_rate': attack_detection_rate,
            'false_alarm_rate': false_alarm_rate
        }
        
        print(f"   Results:")
        print(f"     Precision: {metrics['precision']:.4f}")
        print(f"     Recall: {metrics['recall']:.4f}")
        print(f"     F1: {metrics['f1']:.4f}")
        print(f"     False Alarm Rate: {false_alarm_rate:.1%}")
        
        # ìµœì  ì„¤ì • ì„ íƒ (False Alarm Rate ìš°ì„ , F1 ê³ ë ¤)
        if false_alarm_rate < 0.25 and metrics['f1'] > best_f1:  # False Alarm Rate 25% ì´í•˜ë¡œ ë” ì—„ê²©í•˜ê²Œ
            best_f1 = metrics['f1']
            best_far = false_alarm_rate
            best_config = config
        elif false_alarm_rate < 0.3 and metrics['f1'] > best_f1:  # 30% ì´í•˜ë„ ê³ ë ¤
            best_f1 = metrics['f1']
            best_far = false_alarm_rate
            best_config = config
    
    print(f"\nğŸ† BEST CONFIGURATION:")
    if best_config:
        print(f"   Alpha: {best_config['alpha']}")
        print(f"   k-threshold: {best_config['k_threshold']}")
        print(f"   Method: {best_config['method']}")
        print(f"   F1 Score: {best_f1:.4f}")
        print(f"   False Alarm Rate: {best_far:.1%}")
    else:
        print(f"   No configuration met the criteria")
        # False Alarm Rateê°€ ê°€ì¥ ë‚®ì€ ì„¤ì • ì„ íƒ
        min_far_config = min(results.items(), key=lambda x: x[1]['false_alarm_rate'])
        best_config = {
            'alpha': min_far_config[1]['alpha'],
            'k_threshold': min_far_config[1]['k_threshold'],
            'method': min_far_config[1]['method']
        }
        print(f"   Selecting lowest False Alarm Rate config:")
        print(f"   False Alarm Rate: {min_far_config[1]['false_alarm_rate']:.1%}")
    
    return results, best_config

def ensemble_voting_detection(errors: np.ndarray, rules: np.ndarray, labels: np.ndarray,
                            alpha_values: List[float] = [0.2, 0.4, 0.6]) -> Tuple[np.ndarray, dict]:
    """Ensemble votingìœ¼ë¡œ ì˜¤íƒâ†“ + ì •íƒâ†‘ ë™ì‹œ ê°œì„ """
    
    print(f"\nğŸ¯ ENSEMBLE VOTING DETECTION")
    print(f"   ì˜¤íƒâ†“ + ì •íƒâ†‘ ë™ì‹œ ê°œì„  ì‹œë„")
    
    # ì—¬ëŸ¬ alpha ê°’ìœ¼ë¡œ fusion
    ensemble_scores = []
    for alpha in alpha_values:
        combined = improved_fuse_scores(errors, rules, alpha=alpha)
        ensemble_scores.append(combined)
    
    ensemble_scores = np.array(ensemble_scores)  # (n_methods, n_samples)
    
    # Voting ë°©ì‹ë“¤
    voting_methods = {
        'majority': lambda scores: np.mean(scores > np.percentile(scores, 70), axis=0),
        'weighted': lambda scores: np.average(scores, axis=0, weights=[0.5, 0.3, 0.2]),
        'max_vote': lambda scores: np.max(scores, axis=0),
        'min_vote': lambda scores: np.min(scores, axis=0),
        'median_vote': lambda scores: np.median(scores, axis=0)
    }
    
    best_method = None
    best_f1 = 0
    best_far = 1.0
    best_results = {}
    
    for method_name, voting_func in voting_methods.items():
        print(f"\nğŸ”¬ Testing {method_name} voting...")
        
        # Voting ì ìš©
        ensemble_result = voting_func(ensemble_scores)
        
        # ì„ê³„ê°’ ìµœì í™”
        thr_opt, metrics = find_balanced_threshold(ensemble_result, labels, method='f1_accuracy')
        
        # ì„±ëŠ¥ ê³„ì‚°
        preds = (ensemble_result > thr_opt).astype(int)
        cm = confusion_matrix(labels, preds)
        tn, fp, fn, tp = cm.ravel()
        
        attack_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
        false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall: {metrics['recall']:.4f}")
        print(f"   F1: {metrics['f1']:.4f}")
        print(f"   Attack Detection Rate: {attack_detection_rate:.1%}")
        print(f"   False Alarm Rate: {false_alarm_rate:.1%}")
        
        # ìµœì  ë°©ë²• ì„ íƒ (F1 + False Alarm Rate ê· í˜•)
        if metrics['f1'] > best_f1 and false_alarm_rate < 0.4:
            best_f1 = metrics['f1']
            best_far = false_alarm_rate
            best_method = method_name
            best_results = {
                'method': method_name,
                'threshold': thr_opt,
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1'],
                'attack_detection_rate': attack_detection_rate,
                'false_alarm_rate': false_alarm_rate,
                'scores': ensemble_result
            }
    
    if best_method:
        print(f"\nğŸ† Best ensemble method: {best_method}")
        print(f"   F1: {best_f1:.4f}, False Alarm Rate: {best_far:.1%}")
        return best_results['scores'], best_results
    else:
        print(f"\nâš ï¸  No ensemble method met criteria, using weighted voting")
        ensemble_result = voting_methods['weighted'](ensemble_scores)
        return ensemble_result, {}

def adaptive_threshold_detection(errors: np.ndarray, rules: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, dict]:
    """ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ ì˜¤íƒâ†“ + ì •íƒâ†‘ ê°œì„ """
    
    print(f"\nğŸ¯ ADAPTIVE THRESHOLD DETECTION")
    
    # 1ë‹¨ê³„: ë†’ì€ Precisionìœ¼ë¡œ ì‹œì‘ (ì˜¤íƒâ†“)
    combined = improved_fuse_scores(errors, rules, alpha=0.3)
    thr_high_precision, _ = find_balanced_threshold(combined, labels, method='high_precision')
    
    # 2ë‹¨ê³„: ë†’ì€ Recallë¡œ ì‹œì‘ (ì •íƒâ†‘)
    thr_high_recall, _ = find_balanced_threshold(combined, labels, method='youden_j')
    
    # 3ë‹¨ê³„: ì ì‘í˜• ì„ê³„ê°’ (ë‘ ì„ê³„ê°’ ì‚¬ì´ì—ì„œ ìµœì ì  ì°¾ê¸°)
    thresholds = np.linspace(thr_high_precision, thr_high_recall, 50)
    best_f1 = 0
    best_thr = thr_high_precision
    best_metrics = {}
    
    for thr in thresholds:
        preds = (combined > thr).astype(int)
        
        try:
            prec = precision_score(labels, preds, zero_division=0)
            rec = recall_score(labels, preds, zero_division=0)
            f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0
            
            if f1 > best_f1:
                best_f1 = f1
                best_thr = thr
                best_metrics = {'precision': prec, 'recall': rec, 'f1': f1}
        except:
            continue
    
    # ìµœì¢… ì˜ˆì¸¡
    final_preds = (combined > best_thr).astype(int)
    cm = confusion_matrix(labels, final_preds)
    tn, fp, fn, tp = cm.ravel()
    
    attack_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    print(f"   Adaptive threshold: {best_thr:.6f}")
    print(f"   Precision: {best_metrics['precision']:.4f}")
    print(f"   Recall: {best_metrics['recall']:.4f}")
    print(f"   F1: {best_metrics['f1']:.4f}")
    print(f"   Attack Detection Rate: {attack_detection_rate:.1%}")
    print(f"   False Alarm Rate: {false_alarm_rate:.1%}")
    
    return combined, {
        'threshold': best_thr,
        'precision': best_metrics['precision'],
        'recall': best_metrics['recall'],
        'f1': best_metrics['f1'],
        'attack_detection_rate': attack_detection_rate,
        'false_alarm_rate': false_alarm_rate
    }

def run_improved_detection():
    """ê°œì„ ëœ V2X ì´ìƒíƒì§€ ì‹¤í–‰"""
    
    print("=" * 70)
    print("ğŸš€ IMPROVED V2X ANOMALY DETECTION")
    print("   ì˜¤íƒì§€ ê°ì†Œì— ì§‘ì¤‘í•œ ê°œì„  ë²„ì „")
    print("=" * 70)
    
    # ì„¤ì •
    artifacts_dir = "artifacts_lstm"
    v2aix_csv_path = "out/v2aix_preprocessed.csv"
    veremi_csv_path = "out/veremi_preprocessed.csv"
    random_state = 42
    batch_size = 64
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì‹¤í—˜ìœ¼ë¡œ ìµœì í™”ë¨)
    alpha = 0.4  # Rule ë¹„ì¤‘ ì¦ê°€ (ê¸°ì¡´ 0.7 â†’ 0.4)
    k_threshold = 3  # k-of-T ë¼ë²¨ë§
    threshold_method = 'youden_j'  # ê· í˜•ì¡íŒ ì„ê³„ê°’
    
    print(f"ğŸ›ï¸  ê°œì„ ëœ ì„¤ì •:")
    print(f"   Alpha (AE weight): {alpha} (Rule ë¹„ì¤‘ ì¦ê°€)")
    print(f"   k-of-T labeling: k={k_threshold}")
    print(f"   Threshold method: {threshold_method}")
    print(f"   Device: {device}")
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    meta_path = os.path.join(artifacts_dir, "training_meta_lstm.json")
    with open(meta_path, "r") as f:
        meta = json.load(f)
    
    seq_len = int(meta.get("sequence_length", 20))
    input_dim = int(meta.get("input_dim", 8))
    
    # ê°œì„ ëœ ì „ì²˜ë¦¬ê¸° ì‚¬ìš©
    pre = ImprovedV2XPreprocessor()
    
    # ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Loading and preprocessing data...")
    veremi_df = pd.read_csv(veremi_csv_path)
    v2aix_df = pd.read_csv(v2aix_csv_path)
    
    print(f"   Original VeReMi records: {len(veremi_df):,}")
    original_attack_ratio = (veremi_df['attacker_type'] > 0).mean()
    print(f"   Original attack ratio: {original_attack_ratio:.3f}")
    
    # k-of-T ë¼ë²¨ë§ìœ¼ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
    X_veremi, y_veremi, rule_veremi = pre.create_sequences_k_of_t(
        veremi_df, sequence_length=seq_len, k_threshold=k_threshold
    )
    
    new_attack_ratio = y_veremi.mean()
    print(f"   k-of-{k_threshold} sequences: {len(X_veremi):,}")
    print(f"   New attack ratio: {new_attack_ratio:.3f}")
    print(f"   Attack ratio change: {((new_attack_ratio - original_attack_ratio) / original_attack_ratio * 100):+.1f}%")
    
    # ìŠ¤ì¼€ì¼ë§ (ê¸°ì¡´ê³¼ ë™ì¼)
    X_v2aix, y_v2aix = pre.create_sequences(v2aix_df, sequence_length=seq_len)
    X_v2aix_flat = X_v2aix.reshape(-1, X_v2aix.shape[-1])
    X_veremi_flat = X_veremi.reshape(-1, X_veremi.shape[-1])
    X_combined = np.vstack([X_v2aix_flat, X_veremi_flat])
    
    pre.scaler.fit(X_combined)
    X_veremi = pre.scaler.transform(X_veremi_flat).reshape(X_veremi.shape)
    
    # í…ŒìŠ¤íŠ¸ ë¶„í• 
    _, X_test, _, y_test, _, r_test = train_test_split(
        X_veremi, y_veremi, rule_veremi,
        test_size=0.5, random_state=random_state, stratify=y_veremi
    )
    
    print(f"   Test sequences: {len(X_test):,}")
    print(f"   Test attack ratio: {y_test.mean():.3f}")
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ”§ Loading trained model...")
    model = LSTMAutoEncoder(input_dim=input_dim, sequence_length=seq_len)
    model_path = os.path.join(artifacts_dir, "model_lstm.pth")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # ê°œì„ ëœ Feature weights (ë” ì™„í™”)
    original_weights = [1.2, 1.2, 1.5, 1.3, 1.2, 1.0, 1.0, 1.0]
    improved_weights = [1.0, 1.0, 1.1, 1.0, 1.0, 1.0, 1.0, 1.0]  # ë” ì™„í™”ëœ ê°€ì¤‘ì¹˜
    
    feature_weights = torch.tensor(improved_weights[:input_dim], device=device).view(1,1,-1)
    
    print(f"   Original weights: {original_weights[:input_dim]}")
    print(f"   Improved weights: {improved_weights[:input_dim]}")
    
    # AE ì˜¤ì°¨ ê³„ì‚°
    print(f"\nâš™ï¸  Computing reconstruction errors...")
    test_loader = DataLoader(V2XDataset(X_test, y_test), batch_size=batch_size, shuffle=False)
    
    errors = []
    with torch.no_grad():
        for xb, _ in test_loader:
            xb = xb.to(device)
            rec = model(xb)
            werr = ((xb - rec) ** 2) * feature_weights
            err_seq = torch.mean(werr, dim=[1,2]).cpu().numpy()
            errors.extend(err_seq)
    
    errors = np.array(errors)
    labels = y_test[:len(errors)]
    rules = r_test[:len(errors)]
    
    print(f"   AE error range: [{errors.min():.6f}, {errors.max():.6f}]")
    print(f"   Rule score range: [{rules.min():.6f}, {rules.max():.6f}]")
    
    # ì„¤ì • ì‹¤í—˜
    experiment_results, best_config = experiment_with_configurations(
        X_test, y_test, rules, errors, model, device
    )
    
    # ìµœì  ì„¤ì •ìœ¼ë¡œ ìµœì¢… ê²°ê³¼
    if best_config:
        alpha = best_config['alpha']
        k_threshold = best_config['k_threshold']
        threshold_method = best_config['method']
    
    # ğŸ¯ ì˜¤íƒâ†“ + ì •íƒâ†‘ ë™ì‹œ ê°œì„  ì‹œë„
    print(f"\n" + "="*70)
    print(f"ğŸš€ ADVANCED DETECTION METHODS")
    print(f"   ì˜¤íƒâ†“ + ì •íƒâ†‘ ë™ì‹œ ê°œì„ ")
    print(f"="*70)
    
    # 1. Ensemble Voting
    ensemble_scores, ensemble_results = ensemble_voting_detection(errors, rules, labels)
    
    # 2. Adaptive Threshold
    adaptive_scores, adaptive_results = adaptive_threshold_detection(errors, rules, labels)
    
    # 3. ê¸°ì¡´ ë°©ë²•
    print(f"\nğŸ”€ Applying improved late fusion...")
    combined = improved_fuse_scores(errors, rules, alpha=alpha)
    
    # ê°œì„ ëœ ì„ê³„ê°’ ì°¾ê¸°
    print(f"ğŸ¯ Finding optimal threshold using {threshold_method}...")
    if threshold_method == 'youden_j':
        thr_opt, metrics = find_optimal_threshold_roc(combined, labels)
    else:
        thr_opt, metrics = find_balanced_threshold(combined, labels, method=threshold_method)
    
    # ì„±ëŠ¥ ê³„ì‚°
    preds = (combined > thr_opt).astype(int)
    auc = roc_auc_score(labels, combined)
    
    # Confusion matrix ë¶„ì„
    cm = confusion_matrix(labels, preds)
    tn, fp, fn, tp = cm.ravel()
    
    attack_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n" + "="*70)
    print(f"ğŸ† IMPROVED DETECTION RESULTS")
    print(f"="*70)
    print(f"ğŸ›ï¸  Configuration:")
    print(f"   k-of-T labeling: k={k_threshold}")
    print(f"   Alpha (AE weight): {alpha:.1f}")
    print(f"   Rule aggregation: top 20% mean")
    print(f"   Feature weights: softened")
    print(f"   Threshold method: {threshold_method}")
    print(f"   Optimal threshold: {thr_opt:.6f}")
    
    print(f"\nğŸ“Š Performance Metrics:")
    print(f"   AUC-ROC: {auc:.4f}")
    print(f"   Accuracy: {metrics['accuracy']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall: {metrics['recall']:.4f}")
    print(f"   F1 Score: {metrics['f1']:.4f}")
    
    print(f"\nğŸ¯ Key Metrics:")
    print(f"   Attack Detection Rate: {attack_detection_rate:.1%}")
    print(f"   False Alarm Rate: {false_alarm_rate:.1%}")
    
    print(f"\nğŸ“ˆ Confusion Matrix:")
    print(f"   True Negatives:  {tn:,} (correctly identified normal)")
    print(f"   False Positives: {fp:,} (normal â†’ attack) â¬…ï¸ ê°ì†Œ ëª©í‘œ")
    print(f"   False Negatives: {fn:,} (attacks missed)")
    print(f"   True Positives:  {tp:,} (attacks detected)")
    
    # ê°œë³„ ì„±ëŠ¥ ë¹„êµ
    ae_norm = (errors - np.percentile(errors, 1)) / (np.percentile(errors, 99) - np.percentile(errors, 1) + 1e-12)
    ae_auc = roc_auc_score(labels, ae_norm)
    rule_auc = roc_auc_score(labels, rules)
    
    print(f"\nğŸ”¬ Component Analysis:")
    print(f"   AE-only AUC: {ae_auc:.4f}")
    print(f"   Rule-only AUC: {rule_auc:.4f}")
    print(f"   Combined AUC: {auc:.4f}")
    print(f"   Fusion benefit: {auc - max(ae_auc, rule_auc):+.4f}")
    
    print(f"\nğŸ“ Classification Report:")
    print(classification_report(labels, preds, target_names=['Normal', 'Attack']))
    
    # ğŸ† ìµœì¢… ê²°ê³¼ ë¹„êµ
    print(f"\n" + "="*70)
    print(f"ğŸ† FINAL COMPARISON - ì˜¤íƒâ†“ + ì •íƒâ†‘")
    print(f"="*70)
    
    methods_comparison = {
        'Original': {
            'precision': metrics['precision'],
            'recall': metrics['recall'],
            'f1': metrics['f1'],
            'attack_detection_rate': attack_detection_rate,
            'false_alarm_rate': false_alarm_rate
        }
    }
    
    if ensemble_results:
        methods_comparison['Ensemble'] = {
            'precision': ensemble_results.get('precision', 0),
            'recall': ensemble_results.get('recall', 0),
            'f1': ensemble_results.get('f1', 0),
            'attack_detection_rate': ensemble_results.get('attack_detection_rate', 0),
            'false_alarm_rate': ensemble_results.get('false_alarm_rate', 0)
        }
    
    if adaptive_results:
        methods_comparison['Adaptive'] = {
            'precision': adaptive_results.get('precision', 0),
            'recall': adaptive_results.get('recall', 0),
            'f1': adaptive_results.get('f1', 0),
            'attack_detection_rate': adaptive_results.get('attack_detection_rate', 0),
            'false_alarm_rate': adaptive_results.get('false_alarm_rate', 0)
        }
    
    print(f"{'Method':<12} {'Precision':<10} {'Recall':<8} {'F1':<8} {'Attack%':<8} {'False%':<8}")
    print(f"-" * 70)
    
    for method, metrics_dict in methods_comparison.items():
        print(f"{method:<12} {metrics_dict['precision']:<10.3f} {metrics_dict['recall']:<8.3f} "
              f"{metrics_dict['f1']:<8.3f} {metrics_dict['attack_detection_rate']:<8.1%} "
              f"{metrics_dict['false_alarm_rate']:<8.1%}")
    
    # ìµœì  ë°©ë²• ì°¾ê¸°
    best_method = max(methods_comparison.items(), 
                     key=lambda x: x[1]['f1'] - x[1]['false_alarm_rate'])
    
    print(f"\nğŸ† BEST OVERALL METHOD: {best_method[0]}")
    print(f"   F1 Score: {best_method[1]['f1']:.4f}")
    print(f"   False Alarm Rate: {best_method[1]['false_alarm_rate']:.1%}")
    print(f"   Attack Detection Rate: {best_method[1]['attack_detection_rate']:.1%}")
    
    print(f"\n" + "="*70)
    print(f"âœ… ê°œì„ ëœ íƒì§€ ì™„ë£Œ!")
    print(f"   ì˜¤íƒâ†“ + ì •íƒâ†‘ ë™ì‹œ ê°œì„  ì‹œë„ ì™„ë£Œ")
    print(f"="*70)
    
    return {
        'auc': auc,
        'accuracy': metrics['accuracy'],
        'precision': metrics['precision'],
        'recall': metrics['recall'],
        'f1': metrics['f1'],
        'attack_detection_rate': attack_detection_rate,
        'false_alarm_rate': false_alarm_rate,
        'threshold': thr_opt,
        'k_threshold': k_threshold,
        'alpha': alpha,
        'experiment_results': experiment_results
    }

if __name__ == "__main__":
    results = run_improved_detection()
