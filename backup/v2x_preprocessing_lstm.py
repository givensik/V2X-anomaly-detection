import json
import os
from typing import Dict, List, Tuple, Optional
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from pathlib import Path
# -------------------------------
# Data Preprocessing & Dataset
# -------------------------------

class V2XDataPreprocessor:
    """V2X ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ - V2AIXì™€ VeReMi ë°ì´í„°ì…‹ ëª¨ë‘ì— ì‚¬ìš©"""
    def __init__(self, feature_columns: Optional[List[str]] = None):
        self.feature_columns = feature_columns or [
            'dpos_x', 'dpos_y',          # í”„ë ˆì„ ê°„ ìœ„ì¹˜ ë³€í™” (ë‹¨ìœ„ ì¼ì¹˜ ì´ìŠˆ ì œê±°)
            'dspeed', 'dheading_rad',    # ì†ë„/í—¤ë”© ë³€í™”
            'acceleration',              # dv/dt
            'curvature',                  # yaw_rate/speed (ì•ˆì •ì‹)
            'rel_pos_x', 'rel_pos_y'     # ìƒëŒ€ ìœ„ì¹˜ (ë¡œì»¬ ì¢Œí‘œê³„)
        ]
        self.scaler = StandardScaler()
        self.is_fitted = False

    # ---------- V2AIX ----------
    def extract_cam_features_v2aix(self, json_data: Dict, scenario_id: str = "") -> List[Dict]:
        """V2AIX ë°ì´í„°ì—ì„œ CAM ë©”ì‹œì§€ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        if '/v2x/cam' in json_data:
            for cam_msg in json_data['/v2x/cam']:
                try:
                    cam = cam_msg['message']['cam']
                    cam_params = cam['cam_parameters']
                    basic_container = cam_params['basic_container']
                    ref_pos = basic_container['reference_position']

                    hf_container = cam_params['high_frequency_container']
                    if hf_container['choice'] == 0:
                        vehicle_hf = hf_container['basic_vehicle_container_high_frequency']
                        feature = {
                            'timestamp': cam_msg['recording_timestamp_nsec'],
                            'station_id': cam_msg['message']['header']['station_id']['value'],
                            'pos_x': ref_pos['longitude']['value'] / 1e7,
                            'pos_y': ref_pos['latitude']['value'] / 1e7,
                            'pos_z': ref_pos['altitude']['altitude_value']['value'] / 100,
                            'heading': vehicle_hf['heading']['heading_value']['value'] / 10, # degree
                            'speed': vehicle_hf['speed']['speed_value']['value'] / 100, # m/s
                            'acceleration': vehicle_hf['longitudinal_acceleration']['longitudinal_acceleration_value']['value'] / 10, # m/sÂ²
                            'curvature': vehicle_hf['curvature']['curvature_value']['value'] / 10000, # 1/m
                            'spd_x': 0.0, 'spd_y': 0.0, 'spd_z': 0.0,
                            'scenario_id': scenario_id
                        }
                        heading_rad = np.radians(feature['heading'])
                        feature['spd_x'] = feature['speed'] * np.cos(heading_rad)
                        feature['spd_y'] = feature['speed'] * np.sin(heading_rad)
                        features.append(feature)
                except (KeyError, TypeError):
                    continue
        return features

    # ---------- VeReMi ----------
    def extract_cam_features_veremi(self, json_data: List[Dict], ground_truth: List[Dict], scenario_id: str = "") -> List[Dict]:
        """VeReMi ë°ì´í„°ì—ì„œ CAM ë©”ì‹œì§€ íŠ¹ì„± ì¶”ì¶œ (GroundTruthì™€ ë§¤í•‘)"""
        features = []
        attacker_info = {}
        for gt_entry in ground_truth:
            if gt_entry.get('attackerType', 0) > 0:
                key = (gt_entry['time'], gt_entry['sender'], gt_entry['messageID'])
                attacker_info[key] = gt_entry['attackerType']


        for entry in json_data:
            if entry.get('type') == 3:
                try:
                    key = (entry['sendTime'], entry['sender'], entry['messageID'])
                    is_attacker = key in attacker_info
                    attacker_type = attacker_info.get(key, 0)
                    feature = {
                        'timestamp': entry['sendTime'], # s
                        'station_id': entry['sender'],
                        'pos_x': entry['pos'][0],
                        'pos_y': entry['pos'][1],
                        'pos_z': entry['pos'][2],
                        'spd_x': entry['spd'][0],
                        'spd_y': entry['spd'][1],
                        'spd_z': entry['spd'][2],
                        'heading': np.degrees(np.arctan2(entry['spd'][1], entry['spd'][0])),
                        'speed': float(np.sqrt(entry['spd'][0]**2 + entry['spd'][1]**2)),
                        'acceleration': 0.0, # ì´í›„ Î”ë¡œ ì¬ê³„ì‚°í•˜ì—¬ ì±„ì›€
                        'curvature': 0.0,   # ì´í›„ yaw_rate/speedë¡œ ì±„ì›€
                        'is_attacker': is_attacker,
                        'attacker_type': attacker_type,
                        'scenario_id': scenario_id
                    }
                    features.append(feature)
                except (KeyError, TypeError):
                    continue
        return features

    # ---------- V2AIX Data Loading ----------
    def load_v2aix_data(self, data_path: str, max_files: int = 10) -> pd.DataFrame:
        all_features = []
        if os.path.isfile(data_path):
            json_files = [data_path]
        else:
            json_files = []
            for root, _, files in os.walk(data_path):
                for file in files:
                    if file.endswith('.json') and 'joined' not in file:
                        json_files.append(os.path.join(root, file))
                        if len(json_files) >= max_files:
                            break
                if len(json_files) >= max_files:
                    break

        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # íŒŒì¼ ê²½ë¡œë¥¼ ì‹œë‚˜ë¦¬ì˜¤ IDë¡œ ì‚¬ìš©
                scenario_id = file_path.replace('\\', '/').replace(data_path.replace('\\', '/'), '').strip('/')
                features = self.extract_cam_features_v2aix(data, scenario_id)
                all_features.extend(features)
            except Exception:
                continue

        df = pd.DataFrame(all_features)
        if not df.empty:
            df['dataset'] = 'v2aix'
            df['is_attacker'] = 0
            df['attacker_type'] = 0
        return df
    # ---------- VeReMi Data Loading ----------
    def load_veremi_data(self, json_log_path: str, ground_truth_path: str, 
                        target_attacker_types: Optional[List[int]] = None) -> pd.DataFrame:
        """
        VeReMi ë°ì´í„° ë¡œë”© (íŠ¹ì • ê³µê²© íƒ€ì… í•„í„°ë§ ì§€ì›)
        
        Args:
            json_log_path: JSON ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            ground_truth_path: Ground Truth íŒŒì¼ ê²½ë¡œ  
            target_attacker_types: í¬í•¨í•  ê³µê²© íƒ€ì… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  íƒ€ì… í¬í•¨)
                                 ì˜ˆ: [0, 2] -> ì •ìƒ(0)ê³¼ íƒ€ì…2 ê³µê²©ë§Œ í¬í•¨
        """
        ground_truth = []
        with open(ground_truth_path, 'r') as f:
            for line in f:
                ground_truth.append(json.loads(line.strip()))
        json_data = []
        with open(json_log_path, 'r') as f:
            for line in f:
                json_data.append(json.loads(line.strip()))
        
        # JSON ë¡œê·¸ íŒŒì¼ ê²½ë¡œì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ID ì¶”ì¶œ 
        # ì˜ˆ: .../veins_maat.uc1.14505201.180205_165350/veins-maat/simulations/securecomm2018/results/JSONlog.json 
        # -> veins_maat.uc1.14505201.180205_165350
        scenario_path = Path(json_log_path)
        # results ë””ë ‰í† ë¦¬ì—ì„œ ìµœìƒìœ„ ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬ê¹Œì§€ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ê¸°
        current_path = scenario_path.parent  # results
        while current_path.name not in ['all', 'VeReMi_Data'] and current_path.parent != current_path:
            if current_path.name.startswith('veins_'):  # ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬ íŒ¨í„´
                scenario_id = current_path.name
                break
            current_path = current_path.parent
        else:
            scenario_id = scenario_path.parent.name  # ê¸°ë³¸ê°’
        features = self.extract_cam_features_veremi(json_data, ground_truth, scenario_id)
        df = pd.DataFrame(features)
        
        # VeReMiëŠ” ê°œë³„ íŒŒì¼ì—ì„œ ì¤‘ë³µ ì œê±°í•˜ì§€ ì•Šê³ , ë‚˜ì¤‘ì— ì „ì²´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì œê±°
        
        # íŠ¹ì • ê³µê²© íƒ€ì… í•„í„°ë§
        if not df.empty and target_attacker_types is not None:
            mask = df['attacker_type'].isin(target_attacker_types)
            df = df[mask].reset_index(drop=True)
            print(f"  Filtered to attacker types {target_attacker_types}: {len(df)} records")
        
        if not df.empty:
            df['dataset'] = 'veremi'
        return df
    


    def preprocess_features(self, df: pd.DataFrame, fit_on_mask: Optional[pd.Series] = None) -> pd.DataFrame:
        if df.empty:
            return df
        numeric_features = [c for c in self.feature_columns if c in df.columns]
        df[numeric_features] = df[numeric_features].fillna(0).replace([np.inf, -np.inf], 0)

        # fit_on_maskê°€ ì£¼ì–´ì§€ë©´ ê·¸ ë¶€ë¶„ìœ¼ë¡œë§Œ fit (ì˜ˆ: V2AIX ì •ìƒ)
        if not self.is_fitted:
            fit_idx = fit_on_mask if fit_on_mask is not None else pd.Series(True, index=df.index)
            self.scaler.fit(df.loc[fit_idx, numeric_features].values)
            self.is_fitted = True

        df[numeric_features] = self.scaler.transform(df[numeric_features].values)
        return df


    def create_sequences(self, df: pd.DataFrame, sequence_length: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        numeric_features = [c for c in self.feature_columns if c in df.columns]
        sequences, labels = [], []

        # dataset, scenario_id, station_idê¹Œì§€ í¬í•¨í•´ì„œ ê·¸ë£¹
        for (dataset, scenario_id, station_id), g in df.groupby(['dataset', 'scenario_id', 'station_id'], sort=False):
            station_data = g.sort_values('timestamp')
            if len(station_data) < sequence_length:
                continue
            X = station_data[numeric_features].values
            y = station_data['is_attacker'].values
            for i in range(len(station_data) - sequence_length + 1):
                sequences.append(X[i:i+sequence_length])
                labels.append(y[i:i+sequence_length].max())
        return np.array(sequences), np.array(labels)
    
    # ì‹œí€€ìŠ¤ ìƒì„±ì‹œ ê·œì¹™ ì ìˆ˜ë„ í•¨ê»˜ ë°˜í™˜
    def create_sequences_with_rules(self, df: pd.DataFrame, sequence_length: int = 10
                                ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        AE ì…ë ¥ ì‹œí€€ìŠ¤(X), ë¼ë²¨(y), ê·œì¹™ì ìˆ˜ ì‹œí€€ìŠ¤ ì§‘ê³„(rule_seq_score)ë¥¼ ë°˜í™˜.
        rule_seq_scoreëŠ” ìœˆë„ìš° ë‚´ rule_scoreì˜ 'ìµœëŒ“ê°’'.
        """
        numeric_features = [c for c in self.feature_columns if c in df.columns]
        sequences, labels, rule_scores = [], [], []

        for (dataset, scenario_id, station_id), g in df.groupby(['dataset', 'scenario_id', 'station_id'], sort=False):
            station_data = g.sort_values('timestamp')
            if len(station_data) < sequence_length:
                continue
            X = station_data[numeric_features].values
            y = station_data['is_attacker'].values
            r = station_data['rule_score'].values if 'rule_score' in station_data.columns else np.zeros(len(station_data))
            for i in range(len(station_data) - sequence_length + 1):
                sequences.append(X[i:i+sequence_length])
                labels.append(y[i:i+sequence_length].max())
                rule_scores.append(r[i:i+sequence_length].max())
        return np.array(sequences), np.array(labels), np.array(rule_scores)


    #   ------- Rule-based Scores -------
    def add_dynamic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Î” ê¸°ë°˜ í”¼ì²˜ ê³„ì‚°(+ ìƒëŒ€ ìœ„ì¹˜). ì¢Œí‘œë¥¼ ë¡œì»¬ ë¯¸í„°ë¡œ í†µì¼í•´ì„œ ì‚¬ìš©.
        """
        if df.empty:
            return df

        df = df.sort_values(['dataset','scenario_id','station_id','timestamp']).reset_index(drop=True)
        gkey = ['dataset','scenario_id','station_id']
        prev = df.groupby(gkey).shift(1)

        # --- (A) ë°ì´í„°ì…‹ë³„ dt(ì´ˆ) ---
        # V2AIX: ns -> s, VeReMi: s ê·¸ëŒ€ë¡œ
        is_v2aix = (df['dataset'] == 'v2aix')
        dt = pd.Series(0.0, index=df.index)
        dt.loc[is_v2aix] = (df.loc[is_v2aix,'timestamp'] - prev.loc[is_v2aix,'timestamp']) / 1e9
        dt.loc[~is_v2aix] = (df.loc[~is_v2aix,'timestamp'] - prev.loc[~is_v2aix,'timestamp'])
        dt = dt.replace([np.inf, -np.inf], np.nan).fillna(0.0)

        # --- (B) ë¡œì»¬ ë¯¸í„° ì¢Œí‘œ ë§Œë“¤ê¸° ---
        # V2AIX: pos_x(lon deg), pos_y(lat deg) / VeReMi: ì´ë¯¸ ë¯¸í„° ì¢Œí‘œë¼ê³  ê°€ì •
        pos_mx = pd.Series(0.0, index=df.index)
        pos_my = pd.Series(0.0, index=df.index)

        # ê° ê·¸ë£¹ì˜ ì²« ìœ„ì¹˜(ì›ì )
        first_pos = df.groupby(gkey)[['pos_x','pos_y']].transform('first')

        # V2AIX ê·¸ë£¹: ê²½ìœ„ë„ -> ë¡œì»¬ ë¯¸í„°
        mask = is_v2aix
        if mask.any():
            mx, my = _latlon_to_local_xy(
                lat_deg=df.loc[mask,'pos_y'],
                lon_deg=df.loc[mask,'pos_x'],
                lat0_deg=first_pos.loc[mask,'pos_y'],
                lon0_deg=first_pos.loc[mask,'pos_x']
            )
            pos_mx.loc[mask] = mx.values
            pos_my.loc[mask] = my.values

        # VeReMi ê·¸ë£¹: ì´ë¯¸ ë¯¸í„°ë¡œ ê°€ì • â†’ ì²« ìœ„ì¹˜ë¥¼ (0,0)ìœ¼ë¡œ í‰í–‰ì´ë™
        mask = ~is_v2aix
        if mask.any():
            pos_mx.loc[mask] = (df.loc[mask,'pos_x'] - first_pos.loc[mask,'pos_x']).values
            pos_my.loc[mask] = (df.loc[mask,'pos_y'] - first_pos.loc[mask,'pos_y']).values

        df['pos_mx'] = pos_mx.fillna(0.0)
        df['pos_my'] = pos_my.fillna(0.0)

        # --- (C) Î” ê¸°ë°˜ í”¼ì²˜ (ì „ë¶€ ë¯¸í„° ì¢Œí‘œì—ì„œ) ---
        prev_mx = df.groupby(gkey)['pos_mx'].shift(1).fillna(df['pos_mx'])
        prev_my = df.groupby(gkey)['pos_my'].shift(1).fillna(df['pos_my'])
        dmx = (df['pos_mx'] - prev_mx).fillna(0.0)
        dmy = (df['pos_my'] - prev_my).fillna(0.0)
        df['dpos_x'] = dmx
        df['dpos_y'] = dmy

        dspeed = (df['speed'] - prev['speed']).fillna(0.0)
        df['dspeed'] = dspeed

        prev_h = prev['heading'].fillna(df['heading'])
        dh_deg = (df['heading'] - prev_h).fillna(0.0)
        dh_wrapped = ((dh_deg + 180) % 360) - 180
        dheading_rad = np.radians(dh_wrapped)
        df['dheading_rad'] = dheading_rad

        eps = 1e-6

        # ê°€ì†ë„ a â‰ˆ dspeed / dt
        accel = dspeed / (dt + eps)
        df['acceleration'] = np.where(
            (df['acceleration'] == 0) | (~np.isfinite(df['acceleration'])),
            accel, df['acceleration']
        )

        # yaw_rate & curvature
        yaw_rate = dheading_rad / (dt + eps)     # rad/s
        df['__yaw_rate'] = yaw_rate
        curvature_stable = yaw_rate / (df['speed'].abs() + eps)
        df['curvature'] = np.where(
            (df['curvature'] == 0) | (~np.isfinite(df['curvature'])),
            curvature_stable, df['curvature']
        )

        # ìƒëŒ€ ìœ„ì¹˜(ë¯¸í„° ê¸°ì¤€)
        first_mx = df.groupby(gkey)['pos_mx'].transform('first')
        first_my = df.groupby(gkey)['pos_my'].transform('first')
        df['rel_pos_x'] = (df['pos_mx'] - first_mx).fillna(0.0)
        df['rel_pos_y'] = (df['pos_my'] - first_my).fillna(0.0)

        for c in ['dpos_x','dpos_y','dspeed','dheading_rad','acceleration','curvature','__yaw_rate','rel_pos_x','rel_pos_y','pos_mx','pos_my']:
            df[c] = df[c].replace([np.inf, -np.inf], 0).fillna(0.0)

        return df
    def add_rule_scores(self, df: pd.DataFrame,
                        v_max: float = 40.0,      # 99% ì§€ì  ê¸°ì¤€ (41.6)ë³´ë‹¤ ì•½ê°„ ë‚®ê²Œ
                        a_max: float = 6.0,       # 99% ì§€ì  ê¸°ì¤€ (5.8)ë³´ë‹¤ ì•½ê°„ ë†’ê²Œ  
                        yaw_max: float = 0.5,     # í—¤ë”© ë³€í™”ìœ¨ ìœ ì§€
                        jump_slack: float = 2.0) -> pd.DataFrame:  # ì í”„ í—ˆìš©ì¹˜ ì•½ê°„ ì™„í™”
        if df.empty:
            df['rule_score'] = 0.0
            return df

        df = df.sort_values(['dataset','scenario_id','station_id','timestamp']).reset_index(drop=True)
        prev = df.groupby(['dataset','scenario_id','station_id']).shift(1)

        # ë°ì´í„°ì…‹ë³„ dt(ì´ˆ)
        is_v2aix = (df['dataset'] == 'v2aix')
        dt = pd.Series(0.0, index=df.index)
        dt.loc[is_v2aix] = (df.loc[is_v2aix,'timestamp'] - prev.loc[is_v2aix,'timestamp']) / 1e9
        dt.loc[~is_v2aix] = (df.loc[~is_v2aix,'timestamp'] - prev.loc[~is_v2aix,'timestamp'])
        dt = dt.replace([np.inf, -np.inf], np.nan).fillna(0.0)
        eps = 1e-6
        dt_safe = dt + eps

        # 1) ì†ë„ ìƒí•œ
        v = df['speed'].abs()
        rule_speed = np.clip((v - v_max) / (v_max * 0.2 + eps), 0.0, 1.0)

        # 2) ê°€ì†ë„ ìƒí•œ
        a = df['acceleration'].abs()
        rule_accel = np.clip((a - a_max) / (a_max * 0.25 + eps), 0.0, 1.0)

        # 3) í—¤ë”© ë³€í™”ìœ¨ ìƒí•œ (rad/s)
        yaw_rate = df['__yaw_rate'].abs()
        rule_yaw = np.clip((yaw_rate - yaw_max) / (yaw_max * 0.5 + eps), 0.0, 1.0)

        # 4) ì í”„(ë¯¸í„° ê¸°ì¤€): dposëŠ” ì´ë¯¸ ë¯¸í„°
        dmx = df['dpos_x']; dmy = df['dpos_y']
        jump_dist = np.sqrt(dmx*dmx + dmy*dmy)               # m
        expected = df['speed'].abs() * dt_safe + jump_slack   # m
        rule_jump = np.clip((jump_dist - expected) / (jump_slack + eps), 0.0, 1.0)

        w_speed = w_acc = w_yaw = w_jump = 0.25
        rule_score = (w_speed*rule_speed + w_acc*rule_accel + w_yaw*rule_yaw + w_jump*rule_jump)

        df['rule_speed'] = rule_speed
        df['rule_accel'] = rule_accel
        df['rule_yaw']   = rule_yaw
        df['rule_jump']  = rule_jump
        df['rule_score'] = rule_score.clip(0.0, 1.0)
        return df


 


# -------- Helper to save/load preprocessor --------
def save_preprocessor(pre: V2XDataPreprocessor, path: str):
    import pickle
    with open(path, 'wb') as f:
        pickle.dump({'feature_columns': pre.feature_columns,
                     'scaler': pre.scaler,
                     'is_fitted': pre.is_fitted}, f)

def load_preprocessor(path: str) -> V2XDataPreprocessor:
    import pickle
    with open(path, 'rb') as f:
        data = pickle.load(f)
    pre = V2XDataPreprocessor(feature_columns=data['feature_columns'])
    pre.scaler = data['scaler']
    pre.is_fitted = data['is_fitted']
    return pre

# for VeReMi dataset 
def iter_veremi_pairs(roots, target_attacker_types=None):
    """
    roots: str ë˜ëŠ” str ë¦¬ìŠ¤íŠ¸. ê° ë£¨íŠ¸ ì•„ë˜ë¥¼ ì¬ê·€ íƒìƒ‰í•´ì„œ
           results/**/GroundTruthJSONlog.json ì„ ì°¾ê³ ,
           ê°™ì€ í´ë”ì˜ JSONlog-* ì™€ (log, gt) í˜ì–´ë¥¼ yield.
    target_attacker_types: íŠ¹ì • ê³µê²© íƒ€ì…ì˜ ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì„ íƒ (ì˜ˆ: [2]ëŠ” Type2ë§Œ)
                          Noneì´ë©´ ëª¨ë“  íƒ€ì… í¬í•¨
    """
    if isinstance(roots, (str, Path)):
        roots = [roots]

    seen = set()
    for root in map(Path, roots):
        if not root.exists():
            continue
        # results í´ë”ë§Œ íƒ€ê²Ÿ (ëŒ€/ì†Œë¬¸ì í˜¼ìš© ì•ˆì „)
        for gt in root.rglob("results/GroundTruthJSONlog.json"):
            # ê³µê²© íƒ€ì… í•„í„°ë§: .sca íŒŒì¼ëª…ìœ¼ë¡œ ë””ë ‰í† ë¦¬ í•„í„°ë§
            if target_attacker_types is not None:
                sca_files = list(gt.parent.glob("AttackerType*.sca"))
                if sca_files:
                    # .sca íŒŒì¼ì—ì„œ ê³µê²© íƒ€ì… ì¶”ì¶œ
                    found_types = set()
                    for sca_file in sca_files:
                        # AttackerType2-start=3,0.1-#0.sca -> 2
                        import re
                        match = re.search(r'AttackerType(\d+)-', sca_file.name)
                        if match:
                            found_types.add(int(match.group(1)))
                    
                    # ì›í•˜ëŠ” ê³µê²© íƒ€ì…ì´ ì´ ë””ë ‰í† ë¦¬ì— ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    if not any(att_type in found_types for att_type in target_attacker_types):
                        continue
                else:
                    # .sca íŒŒì¼ì´ ì—†ëŠ” ë””ë ‰í† ë¦¬ëŠ” ì •ìƒ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ê°„ì£¼
                    if 0 not in target_attacker_types:  # ì •ìƒ ë°ì´í„°ë¥¼ ì›í•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
                        continue
            
            # ê°™ì€ ë””ë ‰í† ë¦¬ì˜ ë¡œê·¸ë“¤(ì´ë¦„ ë³€í˜• ë°©ì§€ìš©ìœ¼ë¡œ ë‘ íŒ¨í„´ ì§€ì›)
            candidates = list(gt.parent.glob("JSONlog-*.json")) + \
                         list(gt.parent.glob("*JSONlog*.json"))
            for log in candidates:
                if log.name == "GroundTruthJSONlog.json":
                    continue
                key = (str(log.resolve()), str(gt.resolve()))
                if key in seen:
                    continue
                seen.add(key)
                yield str(log), str(gt)

# -------- Late Fusion of AE and Rule Scores --------
def fuse_scores(ae_seq_scores: np.ndarray,
                rule_seq_scores: np.ndarray,
                alpha: float = 0.7,
                eps: float = 1e-12) -> np.ndarray:
    """
    late-fusion: combined = alpha * ae_norm + (1-alpha) * rule
      - ae_seq_scores: AEì˜ ì‹œí€€ìŠ¤ ì ìˆ˜(ì¬êµ¬ì„± ì˜¤ì°¨ ë“±)
      - rule_seq_scores: ìœ„ì—ì„œ ë§Œë“  ê·œì¹™ ì ìˆ˜(0~1 ê¶Œì¥)
      - alpha: AE ê°€ì¤‘ì¹˜ (0.7 ê¶Œì¥)
    """
    # AE ì ìˆ˜ë¥¼ 0~1 ì •ê·œí™”(95% ë¶„ìœ„ìˆ˜ ê¸°ì¤€ - ì •ìƒ ë°ì´í„° ëŒ€ë¶€ë¶„ì„ ë‚®ì€ ì ìˆ˜ë¡œ)
    ae = np.asarray(ae_seq_scores, dtype=float)
    q95 = np.percentile(ae, 95)  # 95% ë¶„ìœ„ìˆ˜ë¥¼ ìƒí•œìœ¼ë¡œ
    ae_min = ae.min()
    ae_norm = np.clip((ae - ae_min) / (q95 - ae_min + eps), 0.0, 1.0)  # ìµœì†Ÿê°’~95%ë¥¼ 0~1ë¡œ

    rule = np.asarray(rule_seq_scores, dtype=float).clip(0.0, 1.0)
    combined = alpha * ae_norm + (1 - alpha) * rule
    return combined

# ------- Lat/Lon to Local XY (for VeReMi) -------
def _latlon_to_local_xy(lat_deg: pd.Series, lon_deg: pd.Series,
                        lat0_deg: pd.Series, lon0_deg: pd.Series) -> Tuple[pd.Series, pd.Series]:
    """
    ê° í–‰ë§ˆë‹¤ (lat0, lon0)ë¥¼ ì›ì ìœ¼ë¡œ í•˜ëŠ” ê·¼ì‚¬ì  ë¡œì»¬ ENU ë³€í™˜.
    ì‘ì€ ì˜ì—­ ê°€ì •: dx = dlon * cos(lat0) * 111320 (m), dy = dlat * 111320 (m)
    """
    # ì°¨ì´(deg)
    dlat = lat_deg - lat0_deg
    dlon = lon_deg - lon0_deg

    # m/deg
    m_per_deg_lat = 111320.0
    # lat0ëŠ” deg -> rad ë³€í™˜ í›„ cos
    m_per_deg_lon = 111320.0 * np.cos(np.radians(lat0_deg.clip(-89.999, 89.999)))

    x = dlon * m_per_deg_lon
    y = dlat * m_per_deg_lat
    return x, y

if __name__ == "__main__":
    # 1. ì „ì²˜ë¦¬ê¸° ë° íŒŒì¼ ì œí•œ ì„¤ì •
    preprocessor = V2XDataPreprocessor()
    max_veremi_files = 1000  # VeReMi íŒŒì¼ ì²˜ë¦¬ ê°œìˆ˜ ì¦ê°€ (300â†’1000)
    
    # VeReMi ê³µê²© íƒ€ì… ì„¤ì • (ì›í•˜ëŠ” íƒ€ì…ë“¤ì„ ì„ íƒ)
    # 
    # ğŸ¯ ë””ë ‰í† ë¦¬ í•„í„°ë§: íŠ¹ì • ê³µê²© íƒ€ì…ì˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆëŠ” í´ë”ì—ì„œë§Œ ë°ì´í„° ë¡œë“œ
    # ğŸ” ë°ì´í„° í•„í„°ë§: ë¡œë“œëœ ë°ì´í„°ì—ì„œ íŠ¹ì • ê³µê²© íƒ€ì…ë§Œ ì„ íƒ
    #
    # ì‚¬ìš© ì˜ˆì‹œ:
    # None: ëª¨ë“  íƒ€ì… í¬í•¨ (ê¸°ë³¸ê°’)
    # [0]: ì •ìƒ ë°ì´í„°ë§Œ (ì •ìƒ ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬ì—ì„œë§Œ)
    # [2]: íƒ€ì…2 ê³µê²©ë§Œ (AttackerType2*.scaê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ì—ì„œë§Œ)
    # [0, 2]: ì •ìƒ + íƒ€ì…2 ê³µê²© (ì •ìƒ ì‹œë‚˜ë¦¬ì˜¤ + AttackerType2 ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬)
    # [1, 2, 4]: íƒ€ì…1, 2, 4 ê³µê²© (í•´ë‹¹ ê³µê²© íƒ€ì… ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬ë“¤ì—ì„œë§Œ)
    
    directory_filter_types = [1, 2, 4]  # Type 1, 2, 4 ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    data_filter_types = [0, 1, 2, 4]    # ë¡œë“œëœ ë°ì´í„°ì—ì„œ ì •ìƒ + Type 1, 2, 4 ì„ íƒ
    
    print(f"Directory filtering for attacker types: {directory_filter_types}")
    print(f"Data filtering for attacker types: {data_filter_types}")

    # --- 2. V2AIXì™€ VeReMi ë°ì´í„°ë¥¼ ê°ê° ë¡œë“œ ---
    v2aix_path = "V2AIX_Data/json/Mobile/V2X-only"
    print(f"Loading V2AIX data from {v2aix_path}...")
    v2aix_df = preprocessor.load_v2aix_data(v2aix_path, max_files=1000000)

    veremi_root = "VeReMi_Data/all"
    veremi_dfs = []
    print(f"\nLoading VeReMi logs (limit: {max_veremi_files} files)...")
    print(f"Only loading from directories with attack types: {directory_filter_types}")
    
    all_pairs = list(iter_veremi_pairs(veremi_root, target_attacker_types=directory_filter_types))
    print(f"Found {len(all_pairs)} file pairs in filtered directories")
    
    for i, (log_path, gt_path) in enumerate(all_pairs):
        if i >= max_veremi_files:
            break
        file_name = log_path.split('/')[-1] if '/' in log_path else log_path.split('\\')[-1]
        print(f"Processing file {i+1}/{min(len(all_pairs), max_veremi_files)}: {file_name}")
        df = preprocessor.load_veremi_data(log_path, gt_path, data_filter_types)
        if not df.empty:
            veremi_dfs.append(df)

    veremi_df = pd.concat(veremi_dfs, ignore_index=True) if veremi_dfs else pd.DataFrame()

    # VeReMi ì¤‘ë³µ ë©”ì‹œì§€ ì œê±° (ì „ì²´ ë°ì´í„°ì—ì„œ)
    if not veremi_df.empty:
        print(f"Before deduplication: {len(veremi_df)} records")
        # sendTime, sender, scenario_id ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê°™ì€ senderê°€ ê°™ì€ ì‹œê°„ì— ë³´ë‚¸ ë©”ì‹œì§€)
        veremi_df = veremi_df.drop_duplicates(subset=['timestamp', 'station_id', 'scenario_id'], keep='first').reset_index(drop=True)
        print(f"After deduplication: {len(veremi_df)} records")

    # VeReMi ê³µê²© íƒ€ì…ë³„ í†µê³„ ì¶œë ¥
    if not veremi_df.empty:
        print("="*100)
        print(f"\nVeReMi data loaded: {len(veremi_df)} records")
        attacker_stats = veremi_df['attacker_type'].value_counts().sort_index()
        print("Attacker type distribution:")
        for att_type, count in attacker_stats.items():
            att_name = "Normal" if att_type == 0 else f"Attack Type {att_type}"
            print(f"  {att_name}: {count} records ({count/len(veremi_df)*100:.1f}%)")
    
    # --- 3. ë‘ ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° ---
    print("\nCombining V2AIX and VeReMi datasets...")
    combined_df = pd.concat([v2aix_df, veremi_df], ignore_index=True)
    print(f"Total records combined: {len(combined_df)}")
    
    # ì „ì²´ ë°ì´í„°ì…‹ì˜ ê³µê²©/ì •ìƒ ë¹„ìœ¨ ì¶œë ¥
    if not combined_df.empty:
        total_attackers = len(combined_df[combined_df['is_attacker'] == 1])
        total_normal = len(combined_df[combined_df['is_attacker'] == 0])
        print(f"Overall - Normal: {total_normal}, Attackers: {total_attackers}")
        if total_attackers > 0:
            print(f"Attack ratio: {total_attackers/(total_normal+total_attackers)*100:.2f}%")

    # --- 4. ë™ì  í”¼ì²˜ ì¶”ê°€ ---
    print("Adding dynamic features to the combined dataset...")
    combined_df = preprocessor.add_dynamic_features(combined_df)
    # after: combined_df = preprocessor.add_dynamic_features(combined_df)
    print("V2AIX dt/dpos stats:",
        combined_df.loc[combined_df.dataset=="v2aix", ["dpos_x","dpos_y"]].abs().mean().to_dict())

    print("VeReMi dt/dpos stats:",
        combined_df.loc[combined_df.dataset=="veremi", ["dpos_x","dpos_y"]].abs().mean().to_dict())

    # dt, yaw_rate, curvature sanity
    for name, mask in [("V2AIX", combined_df.dataset=="v2aix"),
                    ("VeReMi", combined_df.dataset=="veremi")]:
        dt_guess = combined_df.loc[mask].groupby(["dataset","station_id"])["timestamp"].diff()
        if name=="V2AIX":
            dt_guess = dt_guess/1e9  # ns -> s
        print(name, "dt(s) mean=", float(dt_guess.replace([np.inf,-np.inf], np.nan).dropna().mean()))
        print(name, "yaw_rate mean=", float(combined_df.loc[mask,"__yaw_rate"].abs().mean()))
        print(name, "curvature mean=", float(combined_df.loc[mask,"curvature"].abs().mean()))
    combined_df = preprocessor.add_rule_scores(combined_df)  # â˜… ê·œì¹™ ì ìˆ˜ ì¶”ê°€
    
    print("Dynamic features and rule scores added.")

    # --- 5. ìŠ¤ì¼€ì¼ë§: V2AIX ì •ìƒ ë°ì´í„°ë¡œë§Œ fit  (â˜…ì¤‘ìš”: fit 1íšŒë§Œ!)
    fit_mask = (combined_df['dataset'] == 'v2aix') & (combined_df['is_attacker'] == 0)
    print("Preprocessing (scaling) the combined dataset with V2AIX normals as fit set...")
    processed_df = preprocessor.preprocess_features(combined_df, fit_on_mask=fit_mask)

    # ì‹œí€€ìŠ¤ ë§Œë“¤ê¸° (AE ì…ë ¥ + ë¼ë²¨ + ê·œì¹™ì ìˆ˜ì§‘ê³„)
    X, y, rule_seq = preprocessor.create_sequences_with_rules(processed_df, sequence_length=20)

    # --- 6. ìµœì¢… ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥ ---
    print("Splitting and saving the final preprocessed data...")
    final_v2aix_df = processed_df[processed_df['dataset'] == 'v2aix']
    final_veremi_df = processed_df[processed_df['dataset'] == 'veremi']

    os.makedirs("out", exist_ok=True)
    final_v2aix_df.to_csv("out/v2aix_preprocessed.csv", index=False)
    final_veremi_df.to_csv("out/veremi_preprocessed.csv", index=False)

    print("\nPreprocessing finished successfully.")
    print(f"V2AIX data saved: {len(final_v2aix_df)} rows")
    print(f"VeReMi data saved: {len(final_veremi_df)} rows")

    